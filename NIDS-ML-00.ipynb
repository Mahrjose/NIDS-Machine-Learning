{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data:  (125973, 42)\n",
      "Shape of the test data:  (22544, 42)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "\n",
    "largeDataSet = './data/raw/KDDTrain.arff'\n",
    "smallDataSet = './data/raw/KDDTrain_20Percent.arff'\n",
    "\n",
    "largeTestData = './data/raw/KDDTest.arff'\n",
    "smallTestData = './data/raw/KDDTest-21.arff'\n",
    "\n",
    "# Loading the .arff data format into a pandas dataframe\n",
    "data, meta = arff.loadarff(largeDataSet)\n",
    "test_data, test_meta = arff.loadarff(largeTestData)\n",
    "\n",
    "train = pd.DataFrame(data)\n",
    "test = pd.DataFrame(test_data)\n",
    "\n",
    "print('Shape of the training data: ', train.shape)\n",
    "print('Shape of the test data: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# for feature in categorical_cols:\n",
    "#     plt.figure(figsize=(17, 5)) \n",
    "#     sns.countplot(data=df, x=feature, palette='Set3')\n",
    "#     plt.title(f'Distribution of {feature}')\n",
    "#     plt.xticks(rotation=85)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_outbound_cmds']\n",
      "['num_outbound_cmds']\n",
      "(125973, 42) (22544, 42)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print([col for col in train if train[col].nunique() == 1])\n",
    "print([col for col in test if test[col].nunique() == 1])\n",
    "print(train.shape , test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 41) (22544, 41)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train.drop(columns='num_outbound_cmds')\n",
    "test = test.drop(columns='num_outbound_cmds')\n",
    "print(train.shape , test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape before removing outliers: (125973, 41)\n",
      "Training Dataset Shape after removing outliers: (103446, 41)\n",
      "Test Dataset Shape before removing outliers: (22544, 41)\n",
      "Test Dataset Shape after removing outliers: (15734, 41)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# A threshold value beyond which a data point is considered as an outlier\n",
    "zscore_threshold = 3\n",
    "\n",
    "# Calculate Z-scores for numeric columns (excluding categorical) in the training dataset\n",
    "numeric_columns = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "z_scores_train = np.abs(zscore(train[numeric_columns]))\n",
    "\n",
    "# Create an outlier mask indicating whether each row in the training dataset is an outlier or not\n",
    "outlier_mask_train = np.any(z_scores_train > zscore_threshold, axis=1)\n",
    "\n",
    "# Remove outliers from the training dataset\n",
    "train_original = train.copy()\n",
    "train = train_original[~outlier_mask_train]\n",
    "\n",
    "# Calculate Z-scores for numeric columns (excluding categorical) in the test dataset\n",
    "z_scores_test = np.abs(zscore(test[numeric_columns]))\n",
    "\n",
    "# Create an outlier mask indicating whether each row in the test dataset is an outlier or not\n",
    "outlier_mask_test = np.any(z_scores_test > zscore_threshold, axis=1)\n",
    "\n",
    "# Remove outliers from the test dataset\n",
    "test_original = test.copy()\n",
    "test = test_original[~outlier_mask_test]\n",
    "\n",
    "# Display the shape before and after removing outliers for both datasets\n",
    "print(\"Training Dataset Shape before removing outliers:\", train_original.shape)\n",
    "print(\"Training Dataset Shape after removing outliers:\", train.shape)\n",
    "print(\"Test Dataset Shape before removing outliers:\", test_original.shape)\n",
    "print(\"Test Dataset Shape after removing outliers:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103446, 87) (15734, 87)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_categorical = train[train.select_dtypes(include=['object']).columns]\n",
    "test_categorical = test[test.select_dtypes(include=['object']).columns]\n",
    "\n",
    "train_categorical = train_categorical.drop(columns='class')\n",
    "test_categorical = test_categorical.drop(columns='class')\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(train_categorical)\n",
    "\n",
    "train_encoded = encoder.transform(train_categorical)\n",
    "test_encoded = encoder.transform(test_categorical)\n",
    "\n",
    "print(train_encoded.shape , test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103446, 33) (15734, 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_numerical = train[train.select_dtypes(include=['float64']).columns]\n",
    "test_numerical = test[test.select_dtypes(include=['float64']).columns]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_numerical)\n",
    "test_scaled = scaler.transform(test_numerical)\n",
    "\n",
    "print(train_scaled.shape , test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103446, 120) (15734, 120)\n"
     ]
    }
   ],
   "source": [
    "# train_scaled = pd.DataFrame(train_scaled, columns=train_numerical.columns)\n",
    "# test_scaled = pd.DataFrame(test_scaled, columns=test_numerical.columns)\n",
    "\n",
    "# train_encoded = pd.DataFrame(train_encoded.todense())\n",
    "# test_encoded = pd.DataFrame(test_encoded.todense())\n",
    "\n",
    "# processed_train = pd.concat([train_scaled, train_encoded], axis=1)\n",
    "# processed_test = pd.concat([test_scaled, test_encoded], axis=1)\n",
    "\n",
    "# print(processed_train.shape, processed_test.shape)\n",
    "# processed_train.info()\n",
    "\n",
    "\n",
    "# Convert scaled arrays back to DataFrames with original column names\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=train_numerical.columns)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=test_numerical.columns)\n",
    "\n",
    "# Get the feature names for the encoded columns\n",
    "encoded_columns = encoder.get_feature_names_out(input_features=train_categorical.columns)\n",
    "train_encoded = pd.DataFrame(train_encoded.toarray(), columns=encoded_columns)\n",
    "test_encoded = pd.DataFrame(test_encoded.toarray(), columns=encoded_columns)\n",
    "\n",
    "# Concatenate the scaled and encoded DataFrames\n",
    "processed_train = pd.concat([train_scaled, train_encoded], axis=1)\n",
    "processed_test = pd.concat([test_scaled, test_encoded], axis=1)\n",
    "\n",
    "print(processed_train.shape, processed_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of features in train dataset: 120\n",
      "Initial number of features in test dataset: 120\n",
      "Number of features with high correlation in train dataset: 41\n",
      "Number of remaining features in train dataset: 79\n",
      "Number of remaining features in test dataset: 79\n",
      "(103446, 79) (15734, 79)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation matrix for train dataset\n",
    "corr_matrix_train = processed_train.corr(method='pearson')\n",
    "\n",
    "print(\"Initial number of features in train dataset:\", len(corr_matrix_train.columns))\n",
    "\n",
    "# Calculate the correlation matrix for test dataset\n",
    "corr_matrix_test = processed_test.corr(method='pearson')\n",
    "\n",
    "print(\"Initial number of features in test dataset:\", len(corr_matrix_test.columns))\n",
    "\n",
    "# Define the correlation threshold\n",
    "corr_threshold = 0.4\n",
    "\n",
    "# Find highly correlated features for train dataset\n",
    "high_corr_features_train = set()\n",
    "\n",
    "# Iterate through upper triangle of the correlation matrix for train dataset\n",
    "for i in range(len(corr_matrix_train.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix_train.columns)):\n",
    "        if abs(corr_matrix_train.iloc[i, j]) > corr_threshold:\n",
    "            high_corr_features_train.add(corr_matrix_train.columns[i])\n",
    "            high_corr_features_train.add(corr_matrix_train.columns[j])\n",
    "\n",
    "print(\"Number of features with high correlation in train dataset:\", len(high_corr_features_train))\n",
    "\n",
    "# Drop highly correlated features from both train and test datasets\n",
    "processed_train = processed_train.drop(columns=high_corr_features_train)\n",
    "processed_test = processed_test.drop(columns=high_corr_features_train)\n",
    "\n",
    "print(\"Number of remaining features in train dataset:\", len(processed_train.columns))\n",
    "print(\"Number of remaining features in test dataset:\", len(processed_test.columns))\n",
    "\n",
    "print(processed_train.shape, processed_test.shape)\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# sns.heatmap(processed_train.corr(method='pearson'), fmt='.1g', vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103446, 79) (15734, 79)\n",
      "(103446,) (15734,)\n",
      "[b'anomaly' b'normal']\n",
      "[b'anomaly' b'normal']\n"
     ]
    }
   ],
   "source": [
    "X_train = processed_train.to_numpy()\n",
    "y_train = train['class']\n",
    "\n",
    "X_test = processed_test.to_numpy()\n",
    "y_test = test['class']\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8187364942163468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "y_pred_encoded = knn_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.6317528918266175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_nb_encoded = nb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_nb = accuracy_score(y_test_encoded, y_pred_nb_encoded)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8253463836278124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "dt_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_dt_encoded = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_dt = accuracy_score(y_test_encoded, y_pred_dt_encoded)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6424939621202491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahrjose/bin/BRACU/BRACU-CSE422/Course Project - Network Intrusion Detection using Machine Learning/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_logreg_encoded = logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_logreg = accuracy_score(y_test_encoded, y_pred_logreg_encoded)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
